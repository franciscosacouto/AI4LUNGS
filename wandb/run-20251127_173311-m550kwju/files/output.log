LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name  | Type       | Params
-------------------------------------
0 | model | Sequential | 37.0 K
-------------------------------------
37.0 K    Trainable params
0         Non-trainable params
37.0 K    Total params
0.148     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/nas-ctm01/homes/fmferreira/.conda/envs/ai4lungs/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
/nas-ctm01/homes/fmferreira/.conda/envs/ai4lungs/lib/python3.11/site-packages/torchsurv/loss/cox.py:222: UserWarning: Ties in event time detected; using efron's method to handle ties.
  warnings.warn(
/nas-ctm01/homes/fmferreira/.conda/envs/ai4lungs/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
/nas-ctm01/homes/fmferreira/.conda/envs/ai4lungs/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.






















Epoch 195: 100%|██████████| 2/2 [00:00<00:00, 11.20it/s, v_num=kwju, val_loss=4.040]





Epoch 240: 100%|██████████| 2/2 [00:00<00:00,  8.69it/s, v_num=kwju, val_loss=4.030]

Epoch 252: 100%|██████████| 2/2 [00:00<00:00,  9.59it/s, v_num=kwju, val_loss=4.030]







Epoch 297:   0%|          | 0/2 [00:00<?, ?it/s, v_num=kwju, val_loss=4.020]

Epoch 299: 100%|██████████| 2/2 [00:00<00:00,  6.27it/s, v_num=kwju, val_loss=4.020]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
/nas-ctm01/homes/fmferreira/.conda/envs/ai4lungs/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 226.38it/s]
Test C-index = 0.5751  (95% CI: 0.2891, 0.8611)
Time-dependent AUC(t):
Times: tensor([  90.,  348.,  394.,  491.,  553.,  645.,  649.,  664.,  703.,  819.,
         950.,  964., 1074., 1099., 1101., 1239., 1355., 1732., 1779., 1794.,
        2012., 2064., 2140., 2202., 2216., 2227., 2242., 2252., 2253., 2255.,
        2255., 2260., 2261., 2309., 2318., 2326., 2326., 2338., 2349., 2362.,
        2392., 2394., 2400., 2401., 2405., 2406., 2407., 2429., 2439., 2445.,
        2451., 2452., 2455., 2461., 2474., 2483., 2484., 2489., 2499., 2504.,
        2520., 2531., 2543., 2556., 2561., 2599., 2601., 2607., 2624., 2625.,
        2631., 2632., 2634., 2644., 2648., 2667., 2732., 2760., 2765., 2783.])
AUC: tensor([0.6962, 0.6923, 0.6797, 0.7664, 0.6907, 0.6059, 0.6499, 0.5696, 0.5169,
        0.5676, 0.5984, 0.6149, 0.5716, 0.5670, 0.5344, 0.5413, 0.5728, 0.5880])
Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test_auc_mean       │    0.6124197244644165     │
│        test_cindex        │    0.5751004219055176     │
└───────────────────────────┴───────────────────────────┘